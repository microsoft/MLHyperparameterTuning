{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "In this notebook, we create an AML cluster, and use it to search for the best set of hyperparameters for the model.\n",
    "\n",
    "The steps in this notebook are\n",
    "- [import libraries](#import),\n",
    "- [read in the Azure ML workspace](#workspace),\n",
    "- [create an AML cluster](#cluster),\n",
    "- [upload the data to the cloud](#upload),\n",
    "- [define a hyperparameter search configuration](#configuration),\n",
    "- [create an estimator](#estimator),\n",
    "- [submit the estimator](#submit), and\n",
    "- [get the results](#results).\n",
    "\n",
    "## Imports  <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml.core.VERSION=1.0.41\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration, DataReferenceConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, PrimaryMetricGoal, HyperDriveRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "import azureml.core\n",
    "from get_auth import get_auth\n",
    "print('azureml.core.VERSION={}'.format(azureml.core.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Azure ML workspace <a id='workspace'></a>\n",
    "Read in the the workspace created in a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create Workspace with CLI Authentication\n",
      "Name:\t\thypetuning\n",
      "Location:\teastus2\n"
     ]
    }
   ],
   "source": [
    "auth = get_auth()\n",
    "ws = Workspace.from_config(auth=auth)\n",
    "ws_details = ws.get_details()\n",
    "print('Name:\\t\\t{}\\nLocation:\\t{}'\n",
    "      .format(ws_details['name'],\n",
    "              ws_details['location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an AML cluster <a id='cluster'></a>\n",
    "Define the properties of the cluster needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = 'hypetuning'\n",
    "provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='Standard_D4_v2',\n",
    "        # vm_priority = 'lowpriority', # optional\n",
    "        max_nodes=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the configured cluster if it doesn't already exist, or retrieve it if it does exist. Creation can take about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-existing AML cluster hypetuning\n"
     ]
    }
   ],
   "source": [
    "if cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    if type(compute_target) is not AmlCompute:\n",
    "        raise Exception('Compute target {} is not an AML cluster.'\n",
    "                        .format(cluster_name))\n",
    "    print('Using pre-existing AML cluster {}'.format(cluster_name))\n",
    "else:\n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "\n",
    "    # You can poll for a minimum number of nodes and set a specific timeout. \n",
    "    # If min node count is provided, provisioning will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a detailed view of the cluster.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>currentNodeCount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>targetNodeCount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nodeStateCounts</th>\n",
       "      <td>{'preparingNodeCount': 0, 'runningNodeCount': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocationState</th>\n",
       "      <td>Steady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocationStateTransitionTime</th>\n",
       "      <td>2019-06-01T01:00:34.184000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>errors</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creationTime</th>\n",
       "      <td>2019-05-31T20:46:59.611898+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modifiedTime</th>\n",
       "      <td>2019-05-31T20:47:15.353894+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provisioningState</th>\n",
       "      <td>Succeeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provisioningStateTransitionTime</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaleSettings</th>\n",
       "      <td>{'minNodeCount': 0, 'maxNodeCount': 16, 'nodeI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vmPriority</th>\n",
       "      <td>Dedicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vmSize</th>\n",
       "      <td>STANDARD_D4_V2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Value\n",
       "currentNodeCount                                                                 0\n",
       "targetNodeCount                                                                  0\n",
       "nodeStateCounts                  {'preparingNodeCount': 0, 'runningNodeCount': ...\n",
       "allocationState                                                             Steady\n",
       "allocationStateTransitionTime                     2019-06-01T01:00:34.184000+00:00\n",
       "errors                                                                        None\n",
       "creationTime                                      2019-05-31T20:46:59.611898+00:00\n",
       "modifiedTime                                      2019-05-31T20:47:15.353894+00:00\n",
       "provisioningState                                                        Succeeded\n",
       "provisioningStateTransitionTime                                               None\n",
       "scaleSettings                    {'minNodeCount': 0, 'maxNodeCount': 16, 'nodeI...\n",
       "vmPriority                                                               Dedicated\n",
       "vmSize                                                              STANDARD_D4_V2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(compute_target.get_status().serialize(), name='Value').to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the cloud <a id='upload'></a>\n",
    "Prepare the data in X/y form for AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "train_path = os.path.join(data_path, \"balanced_pairs_train.tsv\")\n",
    "tune_path = os.path.join(data_path, \"balanced_pairs_tune.tsv\")\n",
    "train = pd.read_csv(train_path, sep='\\t', encoding='latin1')\n",
    "tune = pd.read_csv(tune_path, sep='\\t', encoding='latin1')\n",
    "feature_columns = [\"Text_x\", \"Text_y\"]\n",
    "label_column = \"Label\"\n",
    "train_X = (train.Text_x + ' ' + train.Text_y)  # train_X = train[feature_columns]\n",
    "train_y = train[label_column]\n",
    "tune_X = (tune.Text_x + ' ' + tune.Text_y)  # tune_X = tune[feature_columns]\n",
    "tune_y = tune[label_column]\n",
    "train_label_counts = train[label_column].value_counts()\n",
    "train_label_weight = train.shape[0] / (train_label_counts.shape[0] * train_label_counts)\n",
    "train_weight = train[label_column].apply(lambda x: train_label_weight[x])\n",
    "tune_label_counts = tune[label_column].value_counts()\n",
    "tune_label_weight = tune.shape[0] / (tune_label_counts.shape[0] * tune_label_counts)\n",
    "tune_weight = tune[label_column].apply(lambda x: tune_label_weight[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the X/y data out to a data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_data_path = \"automl_data\"\n",
    "os.makedirs(automl_data_path, exist_ok=True)\n",
    "\n",
    "train_X_path = os.path.join(automl_data_path, \"train_X.tsv\")\n",
    "train_X.to_csv(train_X_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "train_y_path = os.path.join(automl_data_path, \"train_y.tsv\")\n",
    "train_y.to_csv(train_y_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "train_weight_path = os.path.join(automl_data_path, \"train_weight.tsv\")\n",
    "train_weight.to_csv(train_weight_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "tune_X_path = os.path.join(automl_data_path, \"tune_X.tsv\")\n",
    "tune_X.to_csv(tune_X_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "tune_y_path = os.path.join(automl_data_path, \"tune_y.tsv\")\n",
    "tune_y.to_csv(tune_y_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "tune_weight_path = os.path.join(automl_data_path, \"tune_weight.tsv\")\n",
    "tune_weight.to_csv(tune_weight_path, sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the data in a particular directory on the workspace's default data store. This will show up in the same location in the file system of every job running on the Batch AI cluster.\n",
    "\n",
    "Get a handle to the workspace's default data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data. We use `overwrite=False` to avoid taking the time to re-upload the data should files with the same names be already present. If you change the data and want to refresh what's uploaded, use `overwrite=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./automl_data/train_X.tsv\n",
      "Uploading ./automl_data/train_weight.tsv\n",
      "Uploading ./automl_data/train_y.tsv\n",
      "Uploading ./automl_data/tune_X.tsv\n",
      "Uploading ./automl_data/tune_weight.tsv\n",
      "Uploading ./automl_data/tune_y.tsv\n",
      "Uploaded ./automl_data/tune_y.tsv, 1 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/train_y.tsv, 2 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/tune_weight.tsv, 3 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/train_weight.tsv, 4 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/tune_X.tsv, 5 files out of an estimated total of 6\n",
      "Uploaded ./automl_data/train_X.tsv, 6 files out of an estimated total of 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_4d1e69a1b0d6466c9daed63dd75f615c"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload(src_dir=os.path.join('.', automl_data_path), target_path='data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data reference to download the data to an absolute location on the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore=\"data\", \n",
    "                   path_on_compute=os.path.join(\"/tmp\", \"azureml\"),\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `get_data.py` file in the `scripts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/get_data.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_data():\n",
    "    automl_data_path = os.path.join(\"/tmp\", \"azureml\", \"data\")\n",
    "    \n",
    "    train_X_path = os.path.join(automl_data_path, \"train_X.tsv\")\n",
    "    train_X = pd.read_csv(train_X_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    train_y_path = os.path.join(automl_data_path, \"train_y.tsv\")\n",
    "    train_y = pd.read_csv(train_y_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    train_weight_path = os.path.join(automl_data_path, \"train_weight.tsv\")\n",
    "    train_weight = pd.read_csv(train_weight_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    tune_X_path = os.path.join(automl_data_path, \"tune_X.tsv\")\n",
    "    tune_X = pd.read_csv(tune_X_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    tune_y_path = os.path.join(automl_data_path, \"tune_y.tsv\")\n",
    "    tune_y = pd.read_csv(tune_y_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    tune_weight_path = os.path.join(automl_data_path, \"tune_weight.tsv\")\n",
    "    tune_weight = pd.read_csv(tune_weight_path, sep='\\t', encoding='latin1').values.flatten()\n",
    "\n",
    "    data = {\n",
    "        \"X\" : train_X, \"y\" : train_y, \"sample_weight\": train_weight,\n",
    "        \"X_valid\": tune_X, \"y_valid\": tune_y, \"sample_weight_valid\": tune_weight \n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to the Linux DSVM\n",
    "conda_run_config.target = compute_target\n",
    "\n",
    "# set the data reference of the run coonfiguration\n",
    "conda_run_config.data_references = {ds.name: dr}\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n",
    "\n",
    "automated_ml_config = AutoMLConfig(task=\"classification\",\n",
    "                                   debug_log=\"dbpedia_auc.log\",\n",
    "                                   path=\"scripts\",\n",
    "                                   data_script=\"get_data.py\",\n",
    "                                   primary_metric=\"AUC_weighted\",\n",
    "                                   run_configuration=conda_run_config,\n",
    "                                   preprocess=True,\n",
    "                                   enable_feature_sweeping=False,\n",
    "                                   iterations=50,\n",
    "                                   iteration_timeout_minutes=90,\n",
    "                                   max_concurrent_iterations=16,\n",
    "                                   max_cores_per_iteration=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the search <a id='submit'></a>\n",
    "Get an experiment to run the search; create it if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='hypetuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the configuration to be run. This should return almost immediately, and the value will be a run object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hypetuning</td><td>AutoML_22e6fd35-8169-45e1-ad62-44ac1f127028</td><td>automl</td><td>Preparing</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/hypetuningauto/providers/Microsoft.MachineLearningServices/workspaces/hypetuning/experiments/hypetuning/runs/AutoML_22e6fd35-8169-45e1-ad62-44ac1f127028\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: hypetuning,\n",
       "Id: AutoML_22e6fd35-8169-45e1-ad62-44ac1f127028,\n",
       "Type: automl,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(automated_ml_config)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment returns a run that when printed shows a table with a link to the `Details Page` in the Azure Portal. That page will let you monitor the status of this run and that of its children runs. By clicking on a particular child run, you can see its details, files output by the script for that configuration, and the logs of the run, including the `driver.log` with the script's print outs.\n",
    "\n",
    "If you want to cancel this trial, run the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the ID of the run in a file. You may use this at a later time to recover the run, as is shown in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = run.id\n",
    "run_id_path = \"run_id.txt\"\n",
    "with open(run_id_path, \"w\") as fp:\n",
    "    fp.write(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until all children runs have either failed or completed, the parent run's status will not be `Completed`. Other possible run statuses include `Preparing`, `Running`, `Finalizing`, and `Failed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preparing'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the RunDetails widget to monitor the execution of the AutoML trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc65e1f02fc492e82c92ed687920e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_AutoMLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the runs to complete. This returns a `dict` with detailed information about the run. Here, we see that the run has `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "CPU times: user 2min 58s, sys: 7.26 s, total: 3min 6s\n",
      "Wall time: 3h 57min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_status = run.wait_for_completion()\n",
    "print(run_status['status'])\n",
    "if run_status['status'] != 'Completed':\n",
    "    raise Exception('The run did not successfully complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best model <a id='results'></a>\n",
    "We can automatically select the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_run = run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best model\n",
    "Read in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(data_path, \"balanced_pairs_test.tsv\")\n",
    "test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "test_X = (test.Text_x + ' ' + test.Text_y)  # test_X = test[feature_columns]\n",
    "test_y = test[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_data_path = \"automl_data\"\n",
    "\n",
    "test_X_path = os.path.join(automl_data_path, \"test_X.tsv\")\n",
    "test_X.to_csv(test_X_path, sep='\\t', header=True, index=False)\n",
    "\n",
    "test_y_path = os.path.join(automl_data_path, \"test_y.tsv\")\n",
    "test_y.to_csv(test_y_path, sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['probabilities'] = best_run[1].predict_proba(test_X.values)[:, 1]\n",
    "\n",
    "# Order the testing data by dupe Id and question AnswerId.\n",
    "group_column = 'Id_x'\n",
    "answerid_column = 'AnswerId_y'\n",
    "test.sort_values([group_column, answerid_column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each dupe probabilities for each question.\n",
    "probabilities = (\n",
    "    test.probabilities\n",
    "    .groupby(test[group_column], sort=False)\n",
    "    .apply(lambda x: tuple(x.values)))\n",
    "\n",
    "# Get the individual records.\n",
    "output_columns_x = ['Id_x', 'AnswerId_x', 'Text_x']\n",
    "test_score = (test[output_columns_x]\n",
    "              .drop_duplicates()\n",
    "              .set_index(group_column))\n",
    "test_score['probabilities'] = probabilities\n",
    "test_score.reset_index(inplace=True)\n",
    "test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score_rank(scores):\n",
    "    \"\"\"Compute the ranks of the scores.\"\"\"\n",
    "    return pd.Series(scores).rank(ascending=False)\n",
    "\n",
    "\n",
    "def label_index(label, label_order):\n",
    "    \"\"\"Compute the index of label in label_order.\"\"\"\n",
    "    loc = np.where(label == label_order)[0]\n",
    "    if loc.shape[0] == 0:\n",
    "        return None\n",
    "    return loc[0]\n",
    "\n",
    "\n",
    "def label_rank(label, scores, label_order):\n",
    "    \"\"\"Compute the rank of the true label given the scores of the question labels.\"\"\"\n",
    "    loc = label_index(label, label_order)\n",
    "    if loc is None:\n",
    "        return len(scores) + 1\n",
    "    return score_rank(scores)[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the model's performance.\")\n",
    "\n",
    "# Collect the ordered AnswerId for computing scores.\n",
    "labels = sorted(train[answerid_column].unique())\n",
    "label_order = pd.DataFrame({'label': labels})\n",
    "\n",
    "# Compute the ranks of the correct answers.\n",
    "test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                       label_rank(x.AnswerId,\n",
    "                                                  x.probabilities,\n",
    "                                                  label_order.label),\n",
    "                                       axis=1)\n",
    "\n",
    "# Compute the number of correctly ranked answers\n",
    "args_rank = 3\n",
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'\n",
    "          .format(i, (test_score['Ranks'] <= i).mean()))\n",
    "mean_rank = test_score['Ranks'].mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scoring dataframe that groups each dupe's probabilities and its AnswerIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank = test.groupby(group_column).apply(lambda x: label_rank(x.AnswerId_x.values, x.probabilities.values, x.AnswerId_y.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_rank = 3\n",
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'\n",
    "          .format(i, (test_rank <= i).mean()))\n",
    "mean_rank = test_rank.mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupe_ranks(x):\n",
    "    y = pd.Series({'AnswerId': x.AnswerId_x.iloc[0],\n",
    "         'probabilities': tuple(x.probabilities.values),\n",
    "         'AnswerIds': tuple(x.AnswerId_y.values),\n",
    "         'Rank': label_rank(x.AnswerId_x.values, x.probabilities.values, x.AnswerId_y.values)\n",
    "        })\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank = test.groupby(group_column).apply(dupe_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank.Rank.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
