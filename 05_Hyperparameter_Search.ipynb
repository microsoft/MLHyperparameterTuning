{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "Perform a hyperparameter search.\n",
    "\n",
    "The steps are\n",
    "- [import libraries and dotenv parameters](#import)\n",
    "- [create a Batch AI client](#client),\n",
    "- [create the Batch AI job configuration parameters](#parameters),\n",
    "- [generate combinations of hyperparameter values](#combinations),\n",
    "- [generate a job for each combination](#jobs),\n",
    "- [run the jobs on Batch AI](#run),\n",
    "- [extract the performance of each combination](#extract),\n",
    "- [identify the combination that had the best performance](#best), and\n",
    "- [use this combination to build and save the best model](#build).\n",
    "\n",
    "## Imports <a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import azure.mgmt.batchai.models as models\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.file import FileService\n",
    "sys.path.append('.')\n",
    "import utilities as utils\n",
    "from utilities.job_factory import ParameterSweep, NumericParameter, DiscreteParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell are the names of various file paths and services shared between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the dotenv file\n",
    "dotenv_path = dotenv.find_dotenv()\n",
    "# The mount point of the Azure file share in the Docker container\n",
    "dotenv.set_key(dotenv_path, 'azure_file_share_mount_path', 'afs')\n",
    "# The mount point of the Azure blob container in the the Docker container\n",
    "dotenv.set_key(dotenv_path, 'azure_blob_mount_path', 'bfs')\n",
    "# The Batch AI experiment\n",
    "dotenv.set_key(dotenv_path, 'experiment_name', 'hyperparameter_search_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the contents of the `.env` file into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Python variables used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_path = os.getenv('configuration_path')\n",
    "image_name = os.getenv('docker_login') + os.getenv('image_repo') + ':latest'\n",
    "azure_blob_container_name = os.getenv('azure_blob_container_name')\n",
    "dataset_path = os.getenv('dataset_path')\n",
    "azure_file_share_name = os.getenv('azure_file_share_name')\n",
    "script_path = os.getenv('script_path')\n",
    "script_name = os.getenv('script_name')\n",
    "cluster_name = os.getenv('cluster_name')\n",
    "azure_blob_mount_path = os.getenv('azure_blob_mount_path')\n",
    "azure_file_share_mount_path = os.getenv('azure_file_share_mount_path')\n",
    "experiment_name = os.getenv('experiment_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Batch AI client <a id='client'></a>\n",
    "Read the configuration, and use it to create a Batch AI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "cfg = utils.config.Configuration(configuration_path)\n",
    "client = utils.config.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters common to all jobs  <a id='parameters'></a>\n",
    "Specify the Docker image used to create the Docker containers that run the experiment's jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_settings = models.ContainerSettings(\n",
    "    image_source_registry=models.ImageSourceRegistry(image=image_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the volumes to be mounted and their mount points in each Docker container's file system. These will give the containers access to the datasets and script, and a location to store job_accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url='https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share_mount_path)\n",
    "    ],\n",
    "    azure_blob_file_systems=[\n",
    "        models.AzureBlobFileSystemReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            container_name=azure_blob_container_name,\n",
    "            relative_mount_path=azure_blob_mount_path)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the locations in a container's file system for\n",
    "- storing the job's standard output and error,\n",
    "- obtaining the datasets, and\n",
    "- storing the job's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_out_err_path_prefix = '$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share_mount_path)\n",
    "\n",
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPT',\n",
    "        path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_blob_mount_path, dataset_path))\n",
    "]\n",
    "\n",
    "output_directories = [\n",
    "    models.OutputDirectory(\n",
    "        id='ALL',\n",
    "        path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share_mount_path))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_file_path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}/{2}'.format(\n",
    "    azure_file_share_mount_path, script_path, script_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the combinations of hyperparameters <a id='combinations'></a>\n",
    "Define specifications for the hyperparameters, and use them to create a parameter substitution object. We choose a single value for the number of estimators that is enough to let us reliably identify the best of the parameter configurations. Once we have the best combination, we will build a model using a larger number of estimators to boost the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_specs = [\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"ESTIMATORS\",\n",
    "        values=[1000]\n",
    "    ),\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"NGRAMS\",\n",
    "        values=list(range(1, 5))\n",
    "    ),\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"MATCH\",\n",
    "        values=list(range(2, 41))\n",
    "    ),\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"MIN_CHILD_SAMPLES\",\n",
    "        values=list(range(1, 31))\n",
    "    ),\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"WEIGHT\",\n",
    "        values=[\"\", \"--unweighted\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "parameters = ParameterSweep(param_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the command line arguments that will be passed to the training script. We will use the parameter substitution object to specify where we would like to substitute the values of the hyperparameters in the command line. Note that `parameters` is used like a dict, with the `parameter_name` being used as the key to specify which parameter to substitute. When the job generation method is called below, the `parameters[name]` variables will be replaced with actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_line_args = '--inputs $AZ_BATCHAI_INPUT_SCRIPT --outputs $AZ_BATCHAI_OUTPUT_ALL'\\\n",
    "    ' --estimators {estimators}'\\\n",
    "    ' --ngrams {ngrams}'\\\n",
    "    ' --match {match}'\\\n",
    "    ' --min_child_samples {min_child_samples}'\\\n",
    "    ' {weight}'.format(\n",
    "    estimators=parameters['ESTIMATORS'],\n",
    "    ngrams=parameters['NGRAMS'],\n",
    "    match=parameters['MATCH'],\n",
    "    min_child_samples=parameters['MIN_CHILD_SAMPLES'],\n",
    "    weight=parameters['WEIGHT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the script path and command line arguments together in a module settings structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_toolkit_settings = models.CustomToolkitSettings(\n",
    "        command_line=' '.join(['python', python_script_file_path, command_line_args]),\n",
    "    )\n",
    "print(custom_toolkit_settings.command_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the jobs for each combination <a id='jobs'></a>\n",
    "Retrieve the cluster information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the information from above to create a job control parameter structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcp = models.JobCreateParameters(\n",
    "    cluster=models.ResourceId(id=cluster.id),\n",
    "    node_count=1,\n",
    "    std_out_err_path_prefix=std_out_err_path_prefix,\n",
    "    input_directories=input_directories,\n",
    "    output_directories=output_directories,\n",
    "    mount_volumes=mount_volumes,\n",
    "    container_settings=container_settings,\n",
    "    custom_toolkit_settings=custom_toolkit_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate a list of jobs to submit, each with a combinations of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_configs = 96\n",
    "jobs_to_submit, param_combinations = parameters.generate_jobs_random_search(jcp, num_configs)\n",
    "print('{:,} configurations.'.format(len(param_combinations)))\n",
    "print('The command line of the first job is\\n{}'.format(jobs_to_submit[0].custom_toolkit_settings.command_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the jobs in an experiment <a id='run'></a>\n",
    "Create a new experiment called `hyperparameter_search_experiment`, and create a job submitter to add jobs to the experiment's job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()\n",
    "experiment_utils = utils.experiment.ExperimentUtils(client, cfg.resource_group, cfg.workspace, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the jobs to the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_jobs = experiment_utils.submit_jobs(jobs_to_submit, 'hyperparam_job2').result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the experiment to complete. This should take about an hour and a half.\n",
    "\n",
    "You can interrupt this cell before all of the experiment's jobs have completed, and later run it again as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = experiment_utils.wait_all_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interrupt the experiment before it's complete, you can delete all its queued and running jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    experiment_utils.delete_jobs_in_experiment(execution_state=models.ExecutionState.queued)\n",
    "    experiment_utils.delete_jobs_in_experiment(execution_state=models.ExecutionState.running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the performance results  <a id='extract'></a>\n",
    "Get a list of successful jobs in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [job\n",
    "        for job in client.jobs.list_by_experiment(cfg.resource_group, cfg.workspace, experiment_name)\n",
    "        if job.execution_state == models.ExecutionState.succeeded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an extractor that pulls desired metric from a job's log file. In this example, we extract the number between \"`INFO:root:Accuracy @3 =`\" and \"`%`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_extractor = utils.job.MetricExtractor(\n",
    "    output_dir_id='ALL',\n",
    "    logfile='TrainTestClassifier.log',\n",
    "    regex='INFO:root:Accuracy @3 = (.*?)\\%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the metric values from the log files of the finished jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "job_accuracies = experiment_utils.get_metrics_for_jobs(jobs, accuracy_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best set of parameters <a id='best'></a>\n",
    "Sort the metrics in decreasing order, and print a summary description of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_accuracies.sort(key=lambda r: r['metric_value'], reverse=True)\n",
    "\n",
    "accuracies = pd.Series({result['job_name']: result['metric_value']\n",
    "                        for result in job_accuracies},\n",
    "                       name='Accuracy @3')\n",
    "accuracies.describe().to_frame().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {ev.name[len('PARAM_'):]: ev.value for ev in job_accuracies[0]['job'].environment_variables}\n",
    "print(\"Best job: {0} with parameters:\".format(job_accuracies[0]['job_name']))\n",
    "pd.Series(best_params, name='Value').to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use them to build the best model <a id='build'></a>\n",
    "Define variables that hold the best combination of parameters, and the number of estimators to use. Typically, increasing the number of estimators will increase the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = 8 * int(best_params['ESTIMATORS'])\n",
    "best_min_child_samples = best_params['MIN_CHILD_SAMPLES']\n",
    "best_match = best_params['MATCH']\n",
    "best_ngrams = best_params['NGRAMS']\n",
    "best_weight = best_params['WEIGHT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training script with the best parameters, and save the model. This should take anywhere from ten minutes to an hour and a half depending on the size of the features determined by the hyperparameters, in particular `match` and `ngrams` (larger is longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -t TrainTestClassifier.py\\\n",
    "    --save\\\n",
    "    --estimators $best_estimators\\\n",
    "    --match $best_match\\\n",
    "    --ngrams $best_ngrams\\\n",
    "    --min_child_samples $best_min_child_samples\\\n",
    "    $best_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tear down the experiment and all related resources go to [the last notebook](06_Tear_Down.ipynb)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MLBatchAIHyperparameterTuning]",
   "language": "python",
   "name": "conda-env-MLBatchAIHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
